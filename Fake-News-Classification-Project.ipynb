{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup & Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # For Feature Engineering\n",
        "from sklearn.linear_model import LogisticRegression # For the Classifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Loading\n",
        "try:\n",
        "    # Load the two separate CSV files\n",
        "    true_df = pd.read_csv('/content/True.csv')\n",
        "    fake_df = pd.read_csv('/content/Fake.csv')\n",
        "    print(\"True.csv and Fake.csv loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"One or both files (True.csv, Fake.csv) not found.\")\n",
        "    print(\"Re-upload the files\")\n",
        "    exit()\n",
        "\n",
        "# 2. Data Merging and Labeling (Crucial Step)\n",
        "# Creating a binary target variable (0 for Fake, 1 for Real)\n",
        "true_df['target'] = 1\n",
        "fake_df['target'] = 0\n",
        "\n",
        "# Combine the two datasets\n",
        "news_df = pd.concat([true_df, fake_df]).reset_index(drop=True)\n",
        "\n",
        "# 3. Data Cleaning and Feature Engineering (The 'text' feature)\n",
        "# Combine 'title' and 'text' to create a richer feature for the classifier\n",
        "news_df['full_text'] = news_df['title'] + \" \" + news_df['text']\n",
        "\n",
        "# Function for basic text cleaning: lowercasing, removing punctuation and special characters\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return text\n",
        "\n",
        "news_df['full_text'] = news_df['title'] + \" \" + news_df['text']\n",
        "news_df['full_text_cleaned'] = news_df['full_text'].apply(clean_text)\n",
        "print(f\"\\nTotal news samples: {len(news_df)}\")\n",
        "print(\"Text cleaning complete.\")"
      ],
      "metadata": {
        "id": "6HnTu7UVlTee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Robust Classification and Evaluation\n",
        "\n",
        "# 1. Feature and Target Split\n",
        "X = news_df['full_text_cleaned']\n",
        "y = news_df['target']\n",
        "\n",
        "# 2. Train-Test Split (Crucial for Validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}. Testing samples: {len(X_test)}\")\n",
        "\n",
        "# 3. Feature Engineering: TF-IDF Vectorization\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) measures the importance of a word.\n",
        "# It is a suitable feature engineering step for text classification.\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Fit the vectorizer on the training data and transform both train and test sets\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Classifier Training\n",
        "# Logistic Regression is a simple yet powerful classifier for text and serves as a strong baseline.\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Prediction and Evaluation (Demonstrating performance)\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate Core Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=['Fake', 'Real'])\n",
        "\n",
        "print(\"\\n--- Model Performance Metrics (Appropriateness of ways performance is measured) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# 6. Visualization: Confusion Matrix (Visual Validation)\n",
        "# The confusion matrix is the best way to validate that the classifier works robustly[cite: 61].\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fake (0)', 'Real (1)'],\n",
        "            yticklabels=['Fake (0)', 'Real (1)'])\n",
        "plt.title('Confusion Matrix: Real vs. Fake News Classifier')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig('/content/tfidf_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAnalysis Summary:\")\n",
        "print(f\"The classifier achieved an accuracy of {accuracy:.4f}, demonstrating a robust ability to distinguish between fake and real news.\")"
      ],
      "metadata": {
        "id": "MtTiuBjwlloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Topic Modeling: NMF for Topic Differences\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Filter Text for Fake News (Target group for analysis)\n",
        "fake_news_df = news_df[news_df['target'] == 0].copy()\n",
        "X_fake_text = fake_news_df['full_text_cleaned']\n",
        "\n",
        "# Step 2: Vectorization for Topic Modeling\n",
        "# Using TF-IDF again, ensuring consistent vectorization across analysis\n",
        "# Max_df=0.8 focuses on words that appear in less than 80% of documents (excluding common words)\n",
        "nmf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "X_fake_tfidf = nmf_vectorizer.fit_transform(X_fake_text)\n",
        "feature_names = nmf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Step 3: Train NMF Model\n",
        "NUM_TOPICS = 7 # Common to select 5-10 topics for interpretability\n",
        "nmf_model = NMF(n_components=NUM_TOPICS, random_state=42, max_iter=300)\n",
        "nmf_model.fit(X_fake_tfidf)\n",
        "\n",
        "# Step 4: Extract and Print Topics\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topic_list = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        topic_list.append(f\"Topic {topic_idx + 1}: {top_words}\")\n",
        "    return topic_list\n",
        "\n",
        "print(\"\\nNMF Topic Modeling Results for Fake News\")\n",
        "fake_topics = display_topics(nmf_model, feature_names, 10) # Showing top 10 words per topic\n",
        "for topic in fake_topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "9eWHJ-Ahmsqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Model 2: Classification using Sentence Embeddings (Advanced/Innovative Model)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load the Sentence Transformer model\n",
        "model_name = 'all-MiniLM-L6-v2' # Efficient model\n",
        "sbert_model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Generate Embeddings for the full dataset\n",
        "print(f\"\\nGenerating embeddings using {model_name}...\")\n",
        "X_embeddings = sbert_model.encode(news_df['full_text_cleaned'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# 3. Split Embeddings (X) and Target (y)\n",
        "X_train_emb, X_test_emb, y_train, y_test = train_test_split(\n",
        "    X_embeddings, news_df['target'], test_size=0.2, random_state=42, stratify=news_df['target']\n",
        ")\n",
        "\n",
        "# 4. Train Classifier on Embeddings (The Model 2 Classification)\n",
        "emb_classifier = LogisticRegression(max_iter=2000, solver='sag', n_jobs=-1)\n",
        "emb_classifier.fit(X_train_emb, y_train)\n",
        "\n",
        "# 5. Prediction and Evaluation\n",
        "y_pred_emb = emb_classifier.predict(X_test_emb)\n",
        "accuracy_emb = accuracy_score(y_test, y_pred_emb)\n",
        "report_emb = classification_report(y_test, y_pred_emb, target_names=['Fake', 'Real'])\n",
        "\n",
        "print(\"\\n--- Model 2 Performance Metrics (Embeddings + LR) ---\")\n",
        "print(f\"Accuracy (Embedding Model): {accuracy_emb:.4f}\")\n",
        "print(\"Classification Report (Embedding Model):\")\n",
        "print(report_emb)\n",
        "\n",
        "# 6. Comparative Conclusion"
      ],
      "metadata": {
        "id": "hTzYsQv6n6DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Final Comparison & Summary Table Code\n",
        "\n",
        "# 1. Extract Key Metrics from Model 1 (TF-IDF)\n",
        "report_lines = report.split('\\n')\n",
        "fake_metrics = report_lines[2].split()\n",
        "real_metrics = report_lines[3].split()\n",
        "\n",
        "# Extract Key Metrics from Model 2 (Embeddings)\n",
        "report_emb_lines = report_emb.split('\\n')\n",
        "fake_metrics_emb = report_emb_lines[2].split()\n",
        "real_metrics_emb = report_emb_lines[3].split()\n",
        "\n",
        "# 2. Creating the Comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Metric': ['Accuracy', 'F1-Score (Fake)', 'F1-Score (Real)', 'Precision (Fake)', 'Recall (Real)'],\n",
        "    'Model 1 (TF-IDF + LR)': [\n",
        "        f\"{accuracy:.4f}\",\n",
        "        fake_metrics[3],\n",
        "        real_metrics[3],\n",
        "        fake_metrics[1],\n",
        "        real_metrics[2]\n",
        "    ],\n",
        "    'Model 2 (Embeddings + LR)': [\n",
        "        f\"{accuracy_emb:.4f}\",\n",
        "        fake_metrics_emb[3],\n",
        "        real_metrics_emb[3],\n",
        "        fake_metrics_emb[1],\n",
        "        real_metrics_emb[2]\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n--- Comparative Model Performance Table (Essential for Paper) ---\")\n",
        "print(comparison_df.to_markdown(index=False))\n"
      ],
      "metadata": {
        "id": "u_12MIPD96_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}